#+TITLE: 使用shell构建多进程的commandlinefu爬虫
#+AUTHOR: lujun9972
#+TAGS: linux和它的小伙伴
#+DATE: [2019-02-13 三 15:03]
#+LANGUAGE:  zh-CN
#+OPTIONS:  H:6 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:nil

[[https://www.commandlinefu.com][commandlinefu]]是一个记录脚本片段的网站,每个片段都有对应的功能说明和对应的标签。我想要做的就是尝试用shell写一个多进程的爬虫把这些代码片段记录在一个org文件中。

* 参数定义

这个脚本需要能够通过 =-n= 参数指定并发的爬虫数(默认为CPU核的数量)，还要能通过 =-f= 指定保存的org文件路径(默认输出到stdout)。
#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
  #!/usr/bin/env bash

  proc_num=$(nproc)
  store_file=/dev/stdout
  while getopts :n:f: OPT; do
      case $OPT in
          n|+n)
              proc_num="$OPTARG"
              ;;
          f|+f)
              store_file="$OPTARG"
              ;;
          ,*)
              echo "usage: ${0##*/} [+-n proc_num] [+-f org_file} [--]"
              exit 2
      esac
  done
  shift $(( OPTIND - 1 ))
  OPTIND=1
#+END_SRC

* 解析命令浏览页面

我们需要一个进程从commandlinefu的浏览列表中抽取各个脚本片段的URL，这个进程将抽取出来的URL存放到一个队列中，再由各个爬虫进程从进程中读取URL并从中抽取出对应的代码片段、描述说明和标签信息写入org文件中。

这里就会遇到三个问题: 
1. 进程之间通讯的队列如何实现
2. 如何从页面中抽取出URL、代码片段、描述说明、标签等信息
3. 多进程对同一文件进行读写时的乱序问题

** 实现进程之间的通讯队列
这个问题比较好解决，我们可以通过一个命名管道来实现
#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
  queue=$(mktemp --dry-run)
  mkfifo ${queue}
  exec 99<>${queue}
  trap "rm ${queue} 2>/dev/null" EXIT
#+END_SRC

** 从页面中抽取想要的信息
从页面中提取元素内容主要有两种方法:
1. 对于简单的HTML页面，我们可以通过 =sed=, =grep=, =awk= 等工具通过正则表达式匹配的方式来从HTML中抽取信息。
2. 通过 [[https://www.w3.org/Tools/HTML-XML-utils/][html-xml-utils]] 工具集中的 [[https://www.w3.org/Tools/HTML-XML-utils/man1/hxselect.html][hxselect]] 来根据CSS selector提取相关元素
 
这里我们使用 =html-xml-utils= 工具来提取
#+BEGIN_SRC shell  :tangle "~/bin/commandlinefu_spider.bash"
  function extract_views_from_browse_page()
  {
      local html="$*"
      echo ${html} |hxclean |hxselect -c -s "\n" "li.list-group-item > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)::attr(href)"
  }

  function extract_nextpage_from_browse_page()
  {
      local html="$*"
      echo ${html} |hxclean |hxselect -s "\n" "li.list-group-item:nth-child(26) > a"|grep '&gt;'|hxselect -c "::attr(href)"
  }
#+END_SRC

这里需要注意的是: *hxselect对html解析时要求遵循严格的XML规范，因此在用hxselect解析之前需要先经过hxclean矫正*

** 循环读取下一页的浏览页面，不断抽取代码片段URL写入队列
这里要解决的是上面提到的第三个问题: *多进程对管道进行读写时如何保障不出现乱序?*
为此，我们需要在写入文件时对文件加锁，然后在写完文件后对文件解锁，在shell中我们可以使用 =flock= 来对文件进行枷锁。
=flock= 有三种使用形式:

1. flock 文件或目录 命令 [参数...]

   这是flock最简单的使用方式, 它会执行 =命令 参数...= 并在命令执行期间对 =文件或路径= 加排他锁，然后在命令执行完毕后再释放该锁.

   但这种形式只能执行单个命令，且无法在命令中进行管道和IO重定向操作

2. flock 文件或目录 -c command
   
   这种方式其实就是 =flock 文件或目录 bash -c "command"= 的简写形式

   这种形式理论上可以在 =command= 中写任意复杂的表达式，但是由于是通过 =bash -c= 来运行的，它的运行环境并不是在当前shell中，因此并不能修改当前shell中的变量

3. flock 文件描述符;commands;flock -u 文件描述符

   这种方式对文件描述符进行加锁，并且在当前shell环境中执行完任意复杂的commands后通过 =-u= 选项对文件描述符进行解锁。
   
   另外，当文件描述符被关闭后，flock也会自动进行解锁。这种方法最灵活，但使用起来不太方便。

*使用flock需要注意几点:*

1. 使用flock对文件加锁的情况下其他程序依然可以在不用flock的情况下写如该文件,比如
   #+BEGIN_SRC shell :results org
     flock /tmp/t sleep 5&
     date |tee -a /tmp/t                # 会执行成功，而且会把date写入到 /tmp/t 中，但
     flock /tmp/t -c "date |tee -a /tmp/t" # 则会等待sleep命令执行完，锁释放后才把date写入到 /tmp/t 中
     echo "------------------------"
     cat /tmp/t
   #+END_SRC
   
   #+RESULTS:
   #+BEGIN_SRC org
   2019年 02月 14日 星期四 13:07:55 HKT
   2019年 02月 14日 星期四 13:08:00 HKT
   ------------------------
   2019年 02月 14日 星期四 13:07:55 HKT
   2019年 02月 14日 星期四 13:08:00 HKT
   #+END_SRC

2. 当flock与管道文件连用时可能会产生死锁，详见[[ego-link:../%E5%BC%82%E9%97%BB%E5%BD%95/flock%E4%B8%8E%E5%91%BD%E5%90%8D%E7%AE%A1%E9%81%93%E8%AF%BB%E5%86%99%E5%BC%95%E5%8F%91%E7%9A%84%E6%AD%BB%E9%94%81.org][flock与管道文件读写引发的死锁]]

#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
  LOCKFILE=(mktemp)
  trap "rm $LOCKFILE 2>/dev/null" EXIT
  exec 3<>$LOCKFILE

  (
      url="commands/browse"
      while [[ -n ${url} ]];do
          echo "从$url中抽取"
          html=$(curl https://www.commandlinefu.com/${url} 2>/dev/null)
          flock -x 0
          extract_views_from_browse_page "${html}" >${queue}
          flock -u 0
          url=$(extract_nextpage_from_browse_page "${html}")
      done
      # 让后面解析代码片段的爬虫进程能够正常退出，而不至于被阻塞.
      for ((i=0;i<${proc_num};i++))
      do
          echo >${queue}
      done
  ) &
#+END_SRC

这里要注意的是， *在找不到下一页URL后，我们用一个for循环往队列里写入了 =proc_num= 个空行*, 这一步的目的是让后面解析代码片段的爬虫进程能够正常退出，而不至于被阻塞.

* 解析脚本片段页面
我们需要从脚本片段的页面中抽取标题、代码片段、描述说明以及标签信息，同时将这些内容按org-mode的格式写入存储文件中.

这里抽取信息的方法跟上面的类似，不过代码片段和描述说明中有一些特舒符号在HTML中是通过转义来表示的，我们还需要转义回来,我们定义一个函数来做这件事情。
#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
  function entity2text()
  {
      cat - |sed '{
        s/&quot;/"/g
        s/&gt;/>/g
        s/&lt;/</g
        s/&amp;/&/g
  }'
  }
#+END_SRC

然后从脚本片段的页面中抽取标题、代码片段、描述说明以及标签信息，同时将这些内容按org-mode的格式写入存储文件中.
#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
    function view_page_to_org_content()
    {
        local url="$1"
        local html=$(curl https://www.commandlinefu.com/${url} 2>/dev/null)
        # headline
        local headline=$(echo ${html} |hxclean |hxselect -c -s "\n" ".col-md-8 > h1:nth-child(1)")
        # command
        local command=$(echo ${html} |hxclean |hxselect -c -s "\n" ".col-md-8 > div:nth-child(2) > span:nth-child(2)"|entity2text)

        # description
        local description=$(echo ${html} |hxclean |hxselect -c -s "\n" ".col-md-8 > div.description"|entity2text)
        # tags
        local tags=$(echo ${html} |hxclean |hxselect -c -s ":" ".functions > a")
        # build org content
        cat <<EOF |flock -x ${store_file} tee -a ${store_file}
  ,* ${headline}      :${tags}
  ${url}
  ${description}
  ,#+begin_src shell
  ${command}
  ,#+end_src

  EOF
    }
#+END_SRC

注意最后输出org-mode的格式并写入存储文件中的代码不要写成下面这样
#+BEGIN_SRC shell
      flock -x ${store_file} cat <<EOF >${store_file}
      ,* ${headline}\t\t ${tags}
      ${description}
      ,#+begin_src shell
      ${command}
      ,#+end_src
  EOF
#+END_SRC
它的意思是使用 =flock= 对 =cat= 命令进行加锁，再把 =flock= 整个命令的结果通过重定向输出到存储文件中，而重定向输出的这个过程是没有加锁的。


* 组合起来
#+BEGIN_SRC shell :tangle "~/bin/commandlinefu_spider.bash"
  for ((i=0;i<${proc_num};i++))
  do
      (
          while :
          do
              flock -x 0
              read -t 1 -u 99 url
              read_return=$?
              flock -u 0
              if [[ ${read_return -ne 0 ]];then
                  sleep 1
                  continue
              fi

              if [[ -z "$url" ]];then
                  break
              fi
              view_page_to_org_content ${url}
          done
  ) &
  done
  wait
#+END_SRC
